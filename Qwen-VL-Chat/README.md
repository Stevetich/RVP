---
studios:
- qwen/Qwen-VL-Chat-Demo
tags:
- Qwen
- multimodal
- OCR
- ç›®æ ‡æ£€æµ‹
- å›¾ç‰‡é—®ç­”
---

<br>

<p align="center">
    <img src="assets/logo.jpg" width="400"/>
<p>
<br>

<p align="center">
        Qwen-VL <a href="https://modelscope.cn/models/qwen/Qwen-VL/summary">ğŸ¤– <a> | <a href="https://huggingface.co/Qwen/Qwen-VL">ğŸ¤—</a>&nbsp ï½œ Qwen-VL-Chat <a href="https://modelscope.cn/models/qwen/Qwen-VL-Chat/summary">ğŸ¤– <a>| <a href="https://huggingface.co/Qwen/Qwen-VL-Chat">ğŸ¤—</a>&nbsp ï½œ &nbsp<a href="https://modelscope.cn/studios/qwen/Qwen-VL-Chat-Demo/summary">Demo</a>&nbsp ï½œ &nbsp<a>Report</a>&nbsp&nbsp | &nbsp&nbsp<a href="https://discord.gg/z3GAxXZ9Ce">Discord</a>
</p>
<br>

**Qwen-VL** æ˜¯é˜¿é‡Œäº‘ç ”å‘çš„å¤§è§„æ¨¡è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLarge Vision Language Model, LVLMï¼‰ã€‚Qwen-VL å¯ä»¥ä»¥å›¾åƒã€æ–‡æœ¬ã€æ£€æµ‹æ¡†ä½œä¸ºè¾“å…¥ï¼Œå¹¶ä»¥æ–‡æœ¬å’Œæ£€æµ‹æ¡†ä½œä¸ºè¾“å‡ºã€‚Qwen-VL ç³»åˆ—æ¨¡å‹çš„ç‰¹ç‚¹åŒ…æ‹¬ï¼š
- **å¼ºå¤§çš„æ€§èƒ½**ï¼šåœ¨å››å¤§ç±»å¤šæ¨¡æ€ä»»åŠ¡çš„æ ‡å‡†è‹±æ–‡æµ‹è¯„ä¸­ï¼ˆZero-shot Caption/VQA/DocVQA/Groundingï¼‰ä¸Šï¼Œå‡å–å¾—åŒç­‰é€šç”¨æ¨¡å‹å¤§å°ä¸‹æœ€å¥½æ•ˆæœï¼›
- **å¤šè¯­è¨€å¯¹è¯æ¨¡å‹**ï¼šå¤©ç„¶æ”¯æŒå¤šè¯­è¨€å¯¹è¯ï¼Œç«¯åˆ°ç«¯æ”¯æŒå›¾ç‰‡é‡Œä¸­è‹±åŒè¯­çš„é•¿æ–‡æœ¬è¯†åˆ«ï¼›
- **å¤šå›¾äº¤é”™å¯¹è¯**ï¼šæ”¯æŒå¤šå›¾è¾“å…¥å’Œæ¯”è¾ƒï¼ŒæŒ‡å®šå›¾ç‰‡é—®ç­”ï¼Œå¤šå›¾æ–‡å­¦åˆ›ä½œç­‰ï¼›
- **é¦–ä¸ªæ”¯æŒä¸­æ–‡å¼€æ”¾åŸŸå®šä½çš„é€šç”¨æ¨¡å‹**ï¼šé€šè¿‡ä¸­æ–‡å¼€æ”¾åŸŸè¯­è¨€è¡¨è¾¾è¿›è¡Œæ£€æµ‹æ¡†æ ‡æ³¨ï¼›
- **ç»†ç²’åº¦è¯†åˆ«å’Œç†è§£**ï¼šç›¸æ¯”äºç›®å‰å…¶å®ƒå¼€æºLVLMä½¿ç”¨çš„224åˆ†è¾¨ç‡ï¼ŒQwen-VLæ˜¯é¦–ä¸ªå¼€æºçš„448åˆ†è¾¨ç‡çš„LVLMæ¨¡å‹ã€‚æ›´é«˜åˆ†è¾¨ç‡å¯ä»¥æå‡ç»†ç²’åº¦çš„æ–‡å­—è¯†åˆ«ã€æ–‡æ¡£é—®ç­”å’Œæ£€æµ‹æ¡†æ ‡æ³¨ã€‚

**Qwen-VL** (Qwen Large Vision Language Model) is the visual multimodal version of the large model series, Qwen (abbr. Tongyi Qianwen), proposed by Alibaba Cloud. Qwen-VL accepts image, text, and bounding box as inputs, outputs text and bounding box. The features of Qwen-VL include:
- **Strong performance**: It significantly surpasses existing open-source Large Vision Language Models (LVLM) under similar scale settings on multiple English evaluation benchmarks (including Zero-shot caption, VQA, DocVQA, and Grounding).
- **Multi-lingual LVLM support text recognization**: Qwen-VL naturally supports multi-lingual conversation, and it promotes end-to-end recognition of Chinese and English bi-lingual text in images.
- **Multi-image interleaved conversations**: This feature allows for the input and comparison of multiple images, as well as the ability to specify questions related to the images and engage in multi-image storytelling.
- **First generalist model support grounding in Chinese**: Detecting bounding boxes through open-domain language expression in both Chinese and English.
- **Fine-grained recognization and understanding**: Compared to the 224 resolution currently used by other open-source LVLM, the 448 resolution promotes fine-grained text recognition, document QA, and bounding box annotation.

ç›®å‰ï¼Œæˆ‘ä»¬æä¾›äº† Qwen-VL ç³»åˆ—çš„ä¸¤ä¸ªæ¨¡å‹ï¼š
- Qwen-VL: Qwen-VL ä»¥ Qwen-7B çš„é¢„è®­ç»ƒæ¨¡å‹ä½œä¸ºè¯­è¨€æ¨¡å‹çš„åˆå§‹åŒ–ï¼Œå¹¶ä»¥ [Openclip ViT-bigG](https://github.com/mlfoundations/open_clip) ä½œä¸ºè§†è§‰ç¼–ç å™¨çš„åˆå§‹åŒ–ï¼Œä¸­é—´åŠ å…¥å•å±‚éšæœºåˆå§‹åŒ–çš„ cross-attentionï¼Œç»è¿‡çº¦1.5Bçš„å›¾æ–‡æ•°æ®è®­ç»ƒå¾—åˆ°ã€‚æœ€ç»ˆå›¾åƒè¾“å…¥åˆ†è¾¨ç‡ä¸º448ã€‚
- Qwen-VL-Chat: åœ¨ Qwen-VL çš„åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬ä½¿ç”¨å¯¹é½æœºåˆ¶æ‰“é€ äº†åŸºäºå¤§è¯­è¨€æ¨¡å‹çš„è§†è§‰AIåŠ©æ‰‹Qwen-VL-Chatï¼Œå…¶è®­ç»ƒæ•°æ®æ¶µç›–äº† QWen-7B çš„çº¯æ–‡æœ¬ SFT æ•°æ®ã€å¼€æº LVLM çš„ SFT æ•°æ®ã€æ•°æ®åˆæˆå’Œäººå·¥æ ‡æ³¨çš„å›¾æ–‡å¯¹é½æ•°æ®ã€‚

å¦‚æœæƒ³äº†è§£æ›´å¤šå…³äºæ¨¡å‹çš„ä¿¡æ¯ï¼Œè¯·ç‚¹å‡»[é“¾æ¥](visual_memo.md)æŸ¥çœ‹æˆ‘ä»¬çš„æŠ€æœ¯å¤‡å¿˜å½•ã€‚

We release two models of the Qwen-VL series:
- Qwen-VL: The pre-trained LVLM model uses Qwen-7B as the initialization of the LLM, and [Openclip ViT-bigG](https://github.com/mlfoundations/open_clip) as the initialization of the visual encoder. And connects them with a randomly initialized cross-attention layer. Qwen-VL was trained on about 1.5B image-text paired data. The final image input resolution is 448.
- Qwen-VL-Chat: A multimodal LLM-based AI assistant, which is trained with alignment techniques.

For more details about Qwen-VL, please refer to our [technical memo](visual_memo.md).

<p align="center">
    <img src="assets/radar.png" width="500"/>
<p>

## ä¾èµ–é¡¹ (Dependency)

* python 3.8åŠä»¥ä¸Šç‰ˆæœ¬
* pytorch 1.12åŠä»¥ä¸Šç‰ˆæœ¬ï¼Œæ¨è2.0åŠä»¥ä¸Šç‰ˆæœ¬
* å»ºè®®ä½¿ç”¨CUDA 11.4åŠä»¥ä¸Šï¼ˆGPUç”¨æˆ·éœ€è€ƒè™‘æ­¤é€‰é¡¹ï¼‰

```bash
pip install modelscope -U
pip install transformers accelerate tiktoken -U
pip install einops transformers_stream_generator -U
pip install "pillow==9.*" -U
pip install torchvision
pip install matplotlib -U
```
## å¿«é€Ÿä½¿ç”¨ï¼ˆQuickstartï¼‰

æ‚¨å¯ä»¥é€šè¿‡ä»¥ä¸‹ä»£ç è½»æ¾è°ƒç”¨ï¼š

You can easily call the model with the following code:

```python
from modelscope import (
    snapshot_download, AutoModelForCausalLM, AutoTokenizer, GenerationConfig
)
import torch
model_id = 'qwen/Qwen-VL-Chat'
revision = 'v1.1.0'

model_dir = snapshot_download(model_id, revision=revision)
torch.manual_seed(1234)

# è¯·æ³¨æ„ï¼šåˆ†è¯å™¨é»˜è®¤è¡Œä¸ºå·²æ›´æ”¹ä¸ºé»˜è®¤å…³é—­ç‰¹æ®Štokenæ”»å‡»é˜²æŠ¤ã€‚
tokenizer = AutoTokenizer.from_pretrained(model_dir, trust_remote_code=True)
# æ‰“å¼€bf16ç²¾åº¦ï¼ŒA100ã€H100ã€RTX3060ã€RTX3070ç­‰æ˜¾å¡å»ºè®®å¯ç”¨ä»¥èŠ‚çœæ˜¾å­˜
# model = AutoModelForCausalLM.from_pretrained(model_dir, device_map="auto", trust_remote_code=True, bf16=True).eval()
# æ‰“å¼€fp16ç²¾åº¦ï¼ŒV100ã€P100ã€T4ç­‰æ˜¾å¡å»ºè®®å¯ç”¨ä»¥èŠ‚çœæ˜¾å­˜
model = AutoModelForCausalLM.from_pretrained(model_dir, device_map="auto", trust_remote_code=True, fp16=True).eval()
# ä½¿ç”¨CPUè¿›è¡Œæ¨ç†ï¼Œéœ€è¦çº¦32GBå†…å­˜
# model = AutoModelForCausalLM.from_pretrained(model_dir, device_map="cpu", trust_remote_code=True).eval()
# é»˜è®¤ä½¿ç”¨è‡ªåŠ¨æ¨¡å¼ï¼Œæ ¹æ®è®¾å¤‡è‡ªåŠ¨é€‰æ‹©ç²¾åº¦
# model = AutoModelForCausalLM.from_pretrained(model_dir, device_map="auto", trust_remote_code=True).eval()

# å¯æŒ‡å®šä¸åŒçš„ç”Ÿæˆé•¿åº¦ã€top_pç­‰ç›¸å…³è¶…å‚
model.generation_config = GenerationConfig.from_pretrained(model_dir, trust_remote_code=True)

# ç¬¬ä¸€è½®å¯¹è¯ 1st dialogue turn
query = tokenizer.from_list_format([
    {'image': 'https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg'},
    {'text': 'è¿™æ˜¯ä»€ä¹ˆ'},
])
response, history = model.chat(tokenizer, query=query, history=None)
print(response)
# å›¾ä¸­æ˜¯ä¸€åå¹´è½»å¥³å­åœ¨æ²™æ»©ä¸Šå’Œå¥¹çš„ç‹—ç©è€ï¼Œç‹—çš„å“ç§æ˜¯æ‹‰å¸ƒæ‹‰å¤šã€‚å¥¹ä»¬ååœ¨æ²™æ»©ä¸Šï¼Œç‹—çš„å‰è…¿æŠ¬èµ·æ¥ï¼Œä¸äººäº’åŠ¨ã€‚

# ç¬¬äºŒè½®å¯¹è¯ 2st dialogue turn
response, history = model.chat(tokenizer, 'è¾“å‡ºå‡»æŒçš„æ£€æµ‹æ¡†', history=history)
print(response)
# <ref>"å‡»æŒ"</ref><box>(211,412),(577,891)</box>
image = tokenizer.draw_bbox_on_latest_picture(response, history)
image.save('output_chat.jpg')
```

<p align="center">
    <img src="assets/demo_highfive.jpg" width="500"/>
<p>

ä½¿ç”¨é‡åŒ–

```python
import os
os.environ['CUDA_VISIBLE_DEVICES'] = '0'
from modelscope import (
    snapshot_download, AutoModelForCausalLM, AutoTokenizer, GenerationConfig,
)
from transformers import BitsAndBytesConfig
import torch
model_id = 'qwen/Qwen-VL-Chat'
revision = 'v1.1.0'

model_dir = snapshot_download(model_id, revision=revision)
torch.manual_seed(1234)
quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.float16,
    bnb_4bit_quant_type='nf4',
    bnb_4bit_use_double_quant=True,
    llm_int8_skip_modules=['lm_head', 'attn_pool.attn'])

tokenizer = AutoTokenizer.from_pretrained(model_dir, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(model_dir, device_map="auto", 
                                             trust_remote_code=True, fp16=True,
                                             quantization_config=quantization_config).eval()
model.generation_config = GenerationConfig.from_pretrained(model_dir, trust_remote_code=True)

query = tokenizer.from_list_format([
    {'image': 'https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg'},
    {'text': 'è¿™æ˜¯ä»€ä¹ˆ'},
])
response, history = model.chat(tokenizer, query=query, history=None)
print(response)

response, history = model.chat(tokenizer, 'è¾“å‡ºç‹—çš„æ£€æµ‹æ¡†', history=history)
print(response)
image = tokenizer.draw_bbox_on_latest_picture(response, history)
image.save('output_chat2.jpg')
```

## å¾®è°ƒ(SFT)

**ä»£ç é“¾æ¥**: https://github.com/modelscope/swift/tree/main/examples/pytorch/llm

1. æ”¯æŒçš„sftæ–¹æ³•: lora, qlora, å…¨å‚æ•°å¾®è°ƒ, ...
2. æ”¯æŒçš„æ¨¡å‹: qwenç³»åˆ—, qwen-vlç³»åˆ—, baichuanç³»åˆ—, chatglm2ç³»åˆ—, llamaç³»åˆ—, openbuddy-llamaç³»åˆ—, internlmç³»åˆ—, xverseç³»åˆ—, ...
3. æ”¯æŒçš„ç‰¹æ€§: æ¨¡å‹é‡åŒ–, DDP, æ¨¡å‹å¹¶è¡Œ, gradient checkpointing, æ¢¯åº¦ç´¯åŠ , æ”¯æŒæ¨é€ModelScope Hub, è‡ªå®šä¹‰æ•°æ®é›†, å¤šæ¨¡æ€å’ŒAgent SFT, å¤šè½®å¯¹è¯, ...

ä½¿ç”¨qlora SFT qwen-vl-chatçš„è„šæœ¬ (éœ€è¦10GBæ˜¾å­˜)
```bash
# https://github.com/modelscope/swift/blob/main/examples/pytorch/llm/scripts/qwen_vl_chat/qlora/sft.sh
# Experimental environment: A10
# 10GB GPU memory (not use flash_attn)
PYTHONPATH=../../.. \
CUDA_VISIBLE_DEVICES=0 \
python llm_sft.py \
    --model_type qwen-vl-chat \
    --sft_type lora \
    --template_type chatml \
    --dtype bf16 \
    --output_dir output \
    --dataset coco-en \
    --train_dataset_sample 20000 \
    --num_train_epochs 1 \
    --max_length 2048 \
    --quantization_bit 4 \
    --bnb_4bit_comp_dtype bf16 \
    --lora_rank 8 \
    --lora_alpha 32 \
    --lora_dropout_p 0. \
    --lora_target_modules c_attn attn.c_proj \
    --gradient_checkpointing true \
    --batch_size 1 \
    --weight_decay 0. \
    --learning_rate 1e-4 \
    --gradient_accumulation_steps 16 \
    --max_grad_norm 0.5 \
    --warmup_ratio 0.03 \
    --eval_steps 100 \
    --save_steps 100 \
    --save_total_limit 2 \
    --logging_steps 10 \
    --use_flash_attn false \
    --push_to_hub false \
    --hub_model_id qwen-vl-chat-qlora \
    --hub_private_repo true \
    --hub_token 'your-sdk-token' \

```


## è¯„æµ‹

æˆ‘ä»¬ä»ä¸¤ä¸ªè§’åº¦è¯„æµ‹äº†ä¸¤ä¸ªæ¨¡å‹çš„èƒ½åŠ›ï¼š
1. åœ¨**è‹±æ–‡æ ‡å‡† Benchmark** ä¸Šè¯„æµ‹æ¨¡å‹çš„åŸºç¡€ä»»åŠ¡èƒ½åŠ›ã€‚ç›®å‰è¯„æµ‹äº†å››å¤§ç±»å¤šæ¨¡æ€ä»»åŠ¡ï¼š
    - Zero-shot Caption: è¯„æµ‹æ¨¡å‹åœ¨æœªè§è¿‡æ•°æ®é›†ä¸Šçš„é›¶æ ·æœ¬å›¾ç‰‡æè¿°èƒ½åŠ›ï¼›
    - General VQA: è¯„æµ‹æ¨¡å‹çš„é€šç”¨é—®ç­”èƒ½åŠ›ï¼Œä¾‹å¦‚åˆ¤æ–­é¢˜ã€é¢œè‰²ã€ä¸ªæ•°ã€ç±»ç›®ç­‰é—®ç­”èƒ½åŠ›ï¼›
    - Text-based VQAï¼šè¯„æµ‹æ¨¡å‹å¯¹äºå›¾ç‰‡ä¸­æ–‡å­—ç›¸å…³çš„è¯†åˆ«/é—®ç­”èƒ½åŠ›ï¼Œä¾‹å¦‚æ–‡æ¡£é—®ç­”ã€å›¾è¡¨é—®ç­”ã€æ–‡å­—é—®ç­”ç­‰ï¼›
    - Referring Expression Compressionï¼šè¯„æµ‹æ¨¡å‹ç»™å®šç‰©ä½“æè¿°ç”»æ£€æµ‹æ¡†çš„èƒ½åŠ›ï¼›

2. **è¯•é‡‘çŸ³ (TouchStone)**ï¼šä¸ºäº†è¯„æµ‹æ¨¡å‹æ•´ä½“çš„å›¾æ–‡å¯¹è¯èƒ½åŠ›å’Œäººç±»å¯¹é½æ°´å¹³ã€‚æˆ‘ä»¬ä¸ºæ­¤æ„å»ºäº†ä¸€ä¸ªåŸºäº GPT4 æ‰“åˆ†æ¥è¯„æµ‹ LVLM æ¨¡å‹çš„ Benchmarkï¼šTouchStoneã€‚åœ¨ TouchStone-v0.1 ä¸­ï¼š
    - è¯„æµ‹åŸºå‡†æ€»è®¡æ¶µç›– 300+å¼ å›¾ç‰‡ã€800+é“é¢˜ç›®ã€27ä¸ªç±»åˆ«ã€‚åŒ…æ‹¬åŸºç¡€å±æ€§é—®ç­”ã€äººç‰©åœ°æ ‡é—®ç­”ã€å½±è§†ä½œå“é—®ç­”ã€è§†è§‰æ¨ç†ã€åäº‹å®æ¨ç†ã€è¯—æ­Œåˆ›ä½œã€æ•…äº‹å†™ä½œï¼Œå•†å“æ¯”è¾ƒã€å›¾ç‰‡è§£é¢˜ç­‰**å°½å¯èƒ½å¹¿æ³›çš„ç±»åˆ«**ã€‚
    - ä¸ºäº†å¼¥è¡¥ç›®å‰ GPT4 æ— æ³•ç›´æ¥è¯»å–å›¾ç‰‡çš„ç¼ºé™·ï¼Œæˆ‘ä»¬ç»™æ‰€æœ‰çš„å¸¦è¯„æµ‹å›¾ç‰‡æä¾›äº†**äººå·¥æ ‡æ³¨çš„å……åˆ†è¯¦ç»†æè¿°**ï¼Œå¹¶ä¸”å°†å›¾ç‰‡çš„è¯¦ç»†æè¿°ã€é—®é¢˜å’Œæ¨¡å‹çš„è¾“å‡ºç»“æœä¸€èµ·äº¤ç»™ GPT4 æ‰“åˆ†ã€‚
    - è¯„æµ‹åŒæ—¶åŒ…å«è‹±æ–‡ç‰ˆæœ¬å’Œä¸­æ–‡ç‰ˆæœ¬ã€‚
  
è¯„æµ‹ç»“æœå¦‚ä¸‹ï¼š

We evaluated the model's ability from two perspectives:
1. **Standard Benchmarks**: We evaluate the model's basic task capabilities on four major categories of multimodal tasks:
   - Zero-shot Caption: Evaluate model's zero-shot image captioning ability on unseen datasets;
   - General VQA: Evaluate the general question-answering ability of pictures, such as the judgment, color, number, category, etc;
   - Text-based VQA: Evaluate the model's ability to recognize text in pictures, such as document QA, chart QA, etc;
   - Referring Expression Comprehension: Evaluate the ability to localize a target object in an image described by a referring expression.

2. **TouchStone**: To evaluate the overall text-image dialogue capability and alignment level with humans, we have constructed a benchmark called TouchStone, which is based on scoring with GPT4 to evaluate the LVLM model.
   - The TouchStone benchmark covers a total of 300+ images, 800+ questions, and 27 categories. Such as attribute-based Q&A, celebrity recognition, writing poetry, summarizing multiple images, product comparison, math problem solving, etc;
   - In order to break the current limitation of GPT4 in terms of direct image input, TouchStone provides fine-grained image annotations by human labeling. These detailed annotations, along with the questions and the model's output, are then presented to GPT4 for scoring.
   - The benchmark includes both English and Chinese versions.

### Zero-shot Captioning & General VQA
<table>
<thead>
  <tr>
    <th rowspan="2">Model type</th>
    <th rowspan="2">Model</th>
    <th colspan="2">Zero-shot Captioning</th>
    <th colspan="5">General VQA</th>
  </tr>
  <tr>
    <th>NoCaps</th>
    <th>Flickr30K</th>
    <th>VQAv2<sup>dev</sup></th>
    <th>OK-VQA</th>
    <th>GQA</th>
    <th>SciQA-Img<br>(0-shot)</th>
    <th>VizWiz<br>(0-shot)</th>
  </tr>
</thead>
<tbody align="center">
  <tr>
    <td rowspan="10">Generalist<br>Models</td>
    <td>Flamingo-9B</td>
    <td>-</td>
    <td>61.5</td>
    <td>51.8</td>
    <td>44.7</td>
    <td>-</td>
    <td>-</td>
    <td>28.8</td>
  </tr>
  <tr>
    <td>Flamingo-80B</td>
    <td>-</td>
    <td>67.2</td>
    <td>56.3</td>
    <td>50.6</td>
    <td>-</td>
    <td>-</td>
    <td>31.6</td>
  </tr>
  <tr>
    <td>Unified-IO-XL</td>
    <td>100.0</td>
    <td>-</td>
    <td>77.9</td>
    <td>54.0</td>
    <td>-</td>
    <td>-</td>
    <td>-</td>
  </tr>
  <tr>
    <td>Kosmos-1</td>
    <td>-</td>
    <td>67.1</td>
    <td>51.0</td>
    <td>-</td>
    <td>-</td>
    <td>-</td>
    <td>29.2</td>
  </tr>
  <tr>
    <td>Kosmos-2</td>
    <td>-</td>
    <td>66.7</td>
    <td>45.6</td>
    <td>-</td>
    <td>-</td>
    <td>-</td>
    <td>-</td>
  </tr>
  <tr>
    <td>BLIP-2 (Vicuna-13B)</td>
    <td>103.9</td>
    <td>71.6</td>
    <td>65.0</td>
    <td>45.9</td>
    <td>32.3</td>
    <td>61.0</td>
    <td>19.6</td>
  </tr>
  <tr>
    <td>InstructBLIP (Vicuna-13B)</td>
    <td><strong>121.9</strong></td>
    <td>82.8</td>
    <td>-</td>
    <td>-</td>
    <td>49.5</td>
    <td>63.1</td>
    <td>33.4</td>
  </tr>
  <tr>
    <td>Shikra (Vicuna-13B)</td>
    <td>-</td>
    <td>73.9</td>
    <td>77.36</td>
    <td>47.16</td>
    <td>-</td>
    <td>-</td>
    <td>-</td>
  </tr>
  <tr>
    <td><strong>Qwen-VL (Qwen-7B)</strong></td>
    <td>121.4</td>
    <td><b>85.8</b></td>
    <td><b>78.8</b></td>
    <td><b>58.6</b></td>
    <td><b>59.3</b></td>
    <td>67.1</td>
    <td>35.2</td>
  </tr>
  <!-- <tr>
    <td>Qwen-VL (4-shot)</td>
    <td>-</td>
    <td>-</td>
    <td>-</td>
    <td>63.6</td>
    <td>-</td>
    <td>-</td>
    <td>39.1</td>
  </tr> -->
  <tr>
    <td>Qwen-VL-Chat</td>
    <td>120.2</td>
    <td>81.0</td>
    <td>78.2</td>
    <td>56.6</td>
    <td>57.5</td>
    <td><b>68.2</b></td>
    <td><b>38.9</b></td>
  </tr>
  <!-- <tr>
    <td>Qwen-VL-Chat (4-shot)</td>
    <td>-</td>
    <td>-</td>
    <td>-</td>
    <td>60.6</td>
    <td>-</td>
    <td>-</td>
    <td>44.45</td>
  </tr> -->
  <tr>
    <td>Previous SOTA<br>(Per Task Fine-tuning)</td>
    <td>-</td>
    <td>127.0<br>(PALI-17B)</td>
    <td>84.5<br>(InstructBLIP<br>-FlanT5-XL)</td>
    <td>86.1<br>(PALI-X<br>-55B)</td>
    <td>66.1<br>(PALI-X<br>-55B)</td>
    <td>72.1<br>(CFR)</td>
    <td>92.53<br>(LLaVa+<br>GPT-4)</td>
    <td>70.9<br>(PALI-X<br>-55B)</td>
  </tr>
</tbody>
</table>

- åœ¨ Zero-shot Caption ä¸­ï¼ŒQwen-VL åœ¨ Flickr30K æ•°æ®é›†ä¸Šå–å¾—äº† **SOTA** çš„ç»“æœï¼Œå¹¶åœ¨ Nocaps æ•°æ®é›†ä¸Šå–å¾—äº†å’Œ InstructBlip å¯ç«äº‰çš„ç»“æœã€‚
- åœ¨ General VQA ä¸­ï¼ŒQwen-VL å–å¾—äº† LVLM æ¨¡å‹åŒç­‰é‡çº§å’Œè®¾å®šä¸‹ **SOTA** çš„ç»“æœã€‚

- For zero-shot image captioning, Qwen-VL achieves the **SOTA** on Flickr30K and competitive results on Nocaps with InstructBlip.
- For general VQA, Qwen-VL achieves the **SOTA** under the same generalist LVLM scale settings.

### Text-oriented VQA (focuse on text understanding capabilities in images)

<table>
<thead>
  <tr>
    <th>Model type</th>
    <th>Model</th>
    <th>TextVQA</th>
    <th>DocVQA</th>
    <th>ChartQA</th>
    <th>AI2D</th>
    <th>OCR-VQA</th>
  </tr>
</thead>
<tbody align="center">
  <tr>
    <td rowspan="5">Generalist Models</td>
    <td>BLIP-2 (Vicuna-13B)</td>
    <td>42.4</td>
    <td>-</td>
    <td>-</td>
    <td>-</td>
    <td>-</td>
  </tr>
  <tr>
    <td>InstructBLIP (Vicuna-13B)</td>
    <td>50.7</td>
    <td>-</td>
    <td>-</td>
    <td>-</td>
    <td>-</td>
  </tr>
  <tr>
    <td>mPLUG-DocOwl (LLaMA-7B)</td>
    <td>52.6</td>
    <td>62.2</td>
    <td>57.4</td>
    <td>-</td>
    <td>-</td>
  </tr>
  <tr>
    <td>Pic2Struct-Large (1.3B)</td>
    <td>-</td>
    <td><b>76.6</b></td>
    <td>58.6</td>
    <td>42.1</td>
    <td>71.3</td>
  </tr>
  <tr>
    <td>Qwen-VL (Qwen-7B)</td>
    <td><b>63.8</b></td>
    <td>65.1</td>
    <td><b>65.7</b></td>
    <td><b>62.3</b></td>
    <td><b>75.7</b></td>
  </tr>
  <tr>
    <td>Specialist SOTAs<br>(Specialist/Finetuned)</td>
    <td>PALI-X-55B (Single-task FT)<br>(Without OCR Pipeline)</td>
    <td>71.44</td>
    <td>80.0</td>
    <td>70.0</td>
    <td>81.2</td>
    <td>75.0</td>
  </tr>
</tbody>
</table>

- åœ¨æ–‡å­—ç›¸å…³çš„è¯†åˆ«/é—®ç­”è¯„æµ‹ä¸Šï¼Œå–å¾—äº†å½“å‰è§„æ¨¡ä¸‹é€šç”¨ LVLM è¾¾åˆ°çš„æœ€å¥½ç»“æœã€‚
- åˆ†è¾¨ç‡å¯¹ä¸Šè¿°æŸå‡ ä¸ªè¯„æµ‹éå¸¸é‡è¦ï¼Œå¤§éƒ¨åˆ† 224 åˆ†è¾¨ç‡çš„å¼€æº LVLM æ¨¡å‹æ— æ³•å®Œæˆä»¥ä¸Šè¯„æµ‹ï¼Œæˆ–åªèƒ½é€šè¿‡åˆ‡å›¾çš„æ–¹å¼è§£å†³ã€‚Qwen-VL å°†åˆ†è¾¨ç‡æå‡åˆ° 448ï¼Œå¯ä»¥ç›´æ¥ä»¥ç«¯åˆ°ç«¯çš„æ–¹å¼è¿›è¡Œä»¥ä¸Šè¯„æµ‹ã€‚Qwen-VL åœ¨å¾ˆå¤šä»»åŠ¡ä¸Šç”šè‡³è¶…è¿‡äº† 1024 åˆ†è¾¨ç‡çš„ Pic2Struct-Large æ¨¡å‹ã€‚

- In text-related recognition/QA evaluation, Qwen-VL achieves the SOTA under the generalist LVLM scale settings.
- Resolution is important for several above evaluations. While most open-source LVLM models with 224 resolution are incapable of these evaluations or can only solve these by cutting images, Qwen-VL scales the resolution to 448 so that it can be evaluated end-to-end. Qwen-VL even outperforms Pic2Struct-Large models of 1024 resolution on some tasks.

### Referring Expression Comprehension
<table>
<thead>
  <tr>
    <th rowspan="2">Model type</th>
    <th rowspan="2">Model</th>
    <th colspan="3">RefCOCO</th>
    <th colspan="3">RefCOCO+</th>
    <th colspan="2">RefCOCOg</th>
    <th>GRIT</th>
  </tr>
  <tr>
    <th>val</th>
    <th>test-A</th>
    <th>test-B</th>
    <th>val</th>
    <th>test-A</th>
    <th>test-B</th>
    <th>val-u</th>
    <th>test-u</th>
    <th>refexp</th>
  </tr>
</thead>
<tbody align="center">
  <tr>
    <td rowspan="8">Generalist Models</td>
    <td>GPV-2</td>
    <td>-</td>
    <td>-</td>
    <td>-</td>
    <td>-</td>
    <td>-</td>
    <td>-</td>
    <td>-</td>
    <td>-</td>
    <td>51.50</td>
  </tr>
  <tr>
    <td>OFA-L*</td>
    <td>79.96</td>
    <td>83.67</td>
    <td>76.39</td>
    <td>68.29</td>
    <td>76.00</td>
    <td>61.75</td>
    <td>67.57</td>
    <td>67.58</td>
    <td>61.70</td>
  </tr>
  <tr>
    <td>Unified-IO</td>
    <td>-</td>
    <td>-</td>
    <td>-</td>
    <td>-</td>
    <td>-</td>
    <td>-</td>
    <td>-</td>
    <td>-</td>
    <td><b>78.61</b></td>
  </tr>
  <tr>
    <td>VisionLLM-H</td>
    <td></td>
    <td>86.70</td>
    <td>-</td>
    <td>-</td>
    <td>-</td>
    <td>-</td>
    <td>-</td>
    <td>-</td>
    <td>-</td>
  </tr>
  <tr>
    <td>Shikra-7B</td>
    <td>87.01</td>
    <td>90.61</td>
    <td>80.24 </td>
    <td>81.60</td>
    <td>87.36</td>
    <td>72.12</td>
    <td>82.27</td>
    <td>82.19</td>
    <td>69.34</td>
  </tr>
  <tr>
    <td>Shikra-13B</td>
    <td>87.83 </td>
    <td>91.11</td>
    <td>81.81</td>
    <td>82.89</td>
    <td>87.79</td>
    <td>74.41</td>
    <td>82.64</td>
    <td>83.16</td>
    <td>69.03</td>
  </tr>
  <tr>
    <td>Qwen-VL-7B</td>
    <td><b>89.36</b></td>
    <td>92.26</td>
    <td><b>85.34</b></td>
    <td><b>83.12</b></td>
    <td>88.25</td>
    <td><b>77.21</b></td>
    <td>85.58</td>
    <td>85.48</td>
    <td>78.22</td>
  </tr>
  <tr>
    <td>Qwen-VL-7B-Chat</td>
    <td>88.55</td>
    <td><b>92.27</b></td>
    <td>84.51</td>
    <td>82.82</td>
    <td><b>88.59</b></td>
    <td>76.79</td>
    <td><b>85.96</b></td>
    <td><b>86.32</b></td>
    <td>-</td>
  <tr>
    <td rowspan="3">Specialist SOTAs<br>(Specialist/Finetuned)</td>
    <td>G-DINO-L</td>
    <td>90.56&nbsp;&nbsp;</td>
    <td>93.19</td>
    <td>88.24</td>
    <td>82.75</td>
    <td>88.95</td>
    <td>75.92</td>
    <td>86.13</td>
    <td>87.02</td>
    <td>-</td>
  </tr>
  <tr>
    <td>UNINEXT-H</td>
    <td>92.64 </td>
    <td>94.33</td>
    <td>91.46</td>
    <td>85.24</td>
    <td>89.63</td>
    <td>79.79</td>
    <td>88.73</td>
    <td>89.37</td>
    <td>-</td>
  </tr>
  <tr>
    <td>ONE-PEACE</td>
    <td>92.58 </td>
    <td>94.18</td>
    <td>89.26</td>
    <td>88.77</td>
    <td>92.21</td>
    <td>83.23</td>
    <td>89.22</td>
    <td>89.27</td>
    <td>-</td>
  </tr>
</tbody>
</table>

- åœ¨å®šä½ä»»åŠ¡ä¸Šï¼ŒQwen-VL å…¨é¢è¶…è¿‡ Shikra-13Bï¼Œå–å¾—äº†ç›®å‰ Generalist LVLM æ¨¡å‹ä¸Šåœ¨ Refcoco ä¸Šçš„ **SOTA**ã€‚
- Qwen-VL å¹¶æ²¡æœ‰åœ¨ä»»ä½•ä¸­æ–‡å®šä½æ•°æ®ä¸Šè®­ç»ƒè¿‡ï¼Œä½†é€šè¿‡ä¸­æ–‡ Caption æ•°æ®å’Œ è‹±æ–‡ Grounding æ•°æ®çš„è®­ç»ƒï¼Œå¯ä»¥ Zero-shot æ³›åŒ–å‡ºä¸­æ–‡ Grounding èƒ½åŠ›ã€‚

- Qwen-VL achieves the **SOTA** in all above referring expression comprehension benchmarks.
- Qwen-VL has not been trained on any Chinese grounding data, but it can still generalize to the Chinese Grounding tasks in a zero-shot way by training Chinese Caption data and English Grounding data.

æˆ‘ä»¬æä¾›äº†ä»¥ä¸Š**æ‰€æœ‰**è¯„æµ‹è„šæœ¬ä»¥ä¾›å¤ç°æˆ‘ä»¬çš„å®éªŒç»“æœã€‚è¯·é˜…è¯» [eval/EVALUATION.md](eval/EVALUATION.md) äº†è§£æ›´å¤šä¿¡æ¯ã€‚

We provide all of the above evaluation scripts for reproducing our experimental results. Please read [eval/EVALUATION.md](eval/EVALUATION.md) for more information.

### Chat èƒ½åŠ›æµ‹è¯„

TouchStone æ˜¯ä¸€ä¸ªåŸºäº GPT4 æ‰“åˆ†æ¥è¯„æµ‹ LVLM æ¨¡å‹çš„å›¾æ–‡å¯¹è¯èƒ½åŠ›å’Œäººç±»å¯¹é½æ°´å¹³çš„åŸºå‡†ã€‚å®ƒæ¶µç›–äº† 300+å¼ å›¾ç‰‡ã€800+é“é¢˜ç›®ã€27ä¸ªç±»åˆ«ï¼ŒåŒ…æ‹¬åŸºç¡€å±æ€§ã€äººç‰©åœ°æ ‡ã€è§†è§‰æ¨ç†ã€è¯—æ­Œåˆ›ä½œã€æ•…äº‹å†™ä½œã€å•†å“æ¯”è¾ƒã€å›¾ç‰‡è§£é¢˜ç­‰**å°½å¯èƒ½å¹¿æ³›çš„ç±»åˆ«**ã€‚å…³äº TouchStone çš„è¯¦ç»†ä»‹ç»ï¼Œè¯·å‚è€ƒè¿™é‡Œ(TODO: Link)ã€‚

TouchStone is a benchmark based on scoring with GPT4 to evaluate the abilities of the LVLM model on text-image dialogue and alignment levels with humans. It covers a total of 300+ images, 800+ questions, and 27 categories, such as attribute-based Q&A, celebrity recognition, writing poetry, summarizing multiple images, product comparison, math problem solving, etc. Please read [eval/EVALUATION.md](eval/EVALUATION.md) for more information.

#### è‹±æ–‡ç‰ˆæœ¬æµ‹è¯„

| Model         | Score |
|---------------|-------|
| PandaGPT      | 488.5 |
| MiniGPT4      | 531.7 |
| InstructBLIP  | 552.4 |
| LLaMA-AdapterV2 | 590.1 |
| mPLUG-Owl     | 605.4 |
| LLaVA         | 602.7 |
| Qwen-VL-Chat   | 645.2 |

#### ä¸­æ–‡ç‰ˆæœ¬æµ‹è¯„

| Model         | Score |
|---------------|-------|
| VisualGLM     | 247.1 |
| Qwen-VL-Chat   | 401.2 |

Qwen-VL-Chat æ¨¡å‹åœ¨ä¸­è‹±æ–‡çš„å¯¹é½è¯„æµ‹ä¸­å‡å–å¾—å½“å‰ LVLM æ¨¡å‹ä¸‹çš„æœ€å¥½ç»“æœã€‚

The Qwen-VL-Chat model has achieved the best results in both Chinese and English alignment evaluation.


## FAQ

å¦‚é‡åˆ°é—®é¢˜ï¼Œæ•¬è¯·æŸ¥é˜…[FAQ](FAQ_zh.md)ä»¥åŠissueåŒºï¼Œå¦‚ä»æ— æ³•è§£å†³å†æäº¤issueã€‚


## ä½¿ç”¨åè®®

ç ”ç©¶äººå‘˜ä¸å¼€å‘è€…å¯ä½¿ç”¨Qwen-VLå’ŒQwen-VL-Chatæˆ–è¿›è¡ŒäºŒæ¬¡å¼€å‘ã€‚æˆ‘ä»¬åŒæ ·å…è®¸å•†ä¸šä½¿ç”¨ï¼Œå…·ä½“ç»†èŠ‚è¯·æŸ¥çœ‹[LICENSE](LICENSE)ã€‚å¦‚éœ€å•†ç”¨ï¼Œè¯·å¡«å†™[é—®å·](https://dashscope.console.aliyun.com/openModelApply/qianwen)ç”³è¯·ã€‚

## è”ç³»æˆ‘ä»¬

å¦‚æœä½ æƒ³ç»™æˆ‘ä»¬çš„ç ”å‘å›¢é˜Ÿå’Œäº§å“å›¢é˜Ÿç•™è¨€ï¼Œè¯·é€šè¿‡é‚®ä»¶ï¼ˆqianwen_opensource@alibabacloud.comï¼‰è”ç³»æˆ‘ä»¬ã€‚

