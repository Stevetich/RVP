{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6a5b5d1-40c6-4f4d-bf88-d612e46d07b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "from skimage.segmentation import slic\n",
    "from fast_slic import Slic\n",
    "\n",
    "from mmseg.datasets import PascalVOCDataset\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21f0abd-8d27-47e5-8c95-29afc04d883e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_slic(im):\n",
    "    segments_slic = slic(\n",
    "        im, n_segments=150, compactness=6, sigma=3.0, start_label=0\n",
    "    ).astype(np.uint8)\n",
    "    # im = Image.fromarray(segments_slic)\n",
    "    \n",
    "    return segments_slic\n",
    "\n",
    "def render_in_img(im, superpixels):\n",
    "    superpixels_ids = np.unique(superpixels)\n",
    "    mask = (superpixels == superpixel_id).astype(np.uint8)\n",
    "    # red_mask = np.zeros_like(im)\n",
    "    # red_mask[:, :, 2] = 255\n",
    "    color = np.array([0,0,255])\n",
    "    color = color[None, None, :]\n",
    "\n",
    "    mask = mask * color\n",
    "    masked_img = cv2.addWeighted(im, 0.7, mask.astype(np.uint8), 0.3, 0)\n",
    "    # os.makedirs(os.path.join(out_base_dir, img_name.split(\".\")[0]), exist_ok=True)\n",
    "    # cv2.imwrite(os.path.join(out_base_dir, img_name.split(\".\")[0], str(superpixel_id)+\".jpg\"), masked_img)\n",
    "    # import ipdb\n",
    "    # ipdb.set_trace()\n",
    "    return masked_img\n",
    "\n",
    "\n",
    "\n",
    "img = cv2.imread(val_dataset[0]['img_path'])\n",
    "# img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "# print (type(im))\n",
    "# im = Image.open(val_dataset[11]['img_path'])\n",
    "# print (type(im))\n",
    "\n",
    "sp1 = slic(img, n_segments=150, compactness=6, sigma=3.0, start_label=0).astype(np.uint8)\n",
    "print (sp.shape)\n",
    "print (img.shape)\n",
    "# im_render = render_in_img(im, im_seg)\n",
    "\n",
    "plt.imshow(sp1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6a7270-b02b-41e6-bb0e-28a0f0ecf3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_dir = '../data/superpixel_img/slic150'\n",
    "sps = os.listdir(sp_dir)\n",
    "sps.sort()\n",
    "\n",
    "# sp = cv2.imread(os.path.join(sp_dir, sps[0]))\n",
    "sp_path = os.path.join(sp_dir, sps[0])\n",
    "sp = Image.open(sp_path)\n",
    "sp = np.array(sp)\n",
    "print (sp.shape)\n",
    "plt.imshow(sp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e864e337-3483-43bd-afe3-159b460ea32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_dir = '../data/superpixel_img/slic150'\n",
    "sps = os.listdir(sp_dir)\n",
    "sps.sort()\n",
    "\n",
    "sp_path = os.path.join(sp_dir, sps[0])\n",
    "sp = cv2.imread(sp_path) # B G R\n",
    "# sp = Image.open(sp_path)\n",
    "# sp = np.array(sp)\n",
    "\n",
    "img_path = os.path.join('../data/VOCdevkit/VOC2012/JPEGImages', sps[0])\n",
    "img = cv2.imread(img_path) # B G R\n",
    "\n",
    "sp_ids = np.unique(sp)\n",
    "ms = []\n",
    "for id in sp_ids:\n",
    "    m = (sp == id)\n",
    "    ms.append(torch.tensor(m))\n",
    "ms = torch.stack(ms, dim=0)\n",
    "ms = ms.permute(0,3,1,2)\n",
    "\n",
    "color = torch.tensor([[0,0,255], [0,255,0], [255,0,0]]) # R G B\n",
    "# Red = color[0].reshape(1, 3, 1, 1)\n",
    "# Green = color[1].reshape(1, 3, 1, 1)\n",
    "Blue = color[2].reshape(1, 3, 1, 1)\n",
    "\n",
    "img_repeated = torch.tensor(img).unsqueeze(0).permute(0,3,1,2).expand_as(ms)\n",
    "img_re = img_repeated * 0.6 + ms * Blue * 0.4\n",
    "img_re.shape\n",
    "sp_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1cd824-c715-4fae-874f-1dc1fe47e696",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch(batch_files, reference_adv_dir, transform):\n",
    "    batch = []\n",
    "    gt_labels = []\n",
    "    tar_labels = []\n",
    "    imgs = []\n",
    "    \n",
    "    for file in batch_files:\n",
    "        file_split = file.split('_')\n",
    "        img_name = file_split[0]\n",
    "\n",
    "        gt_labels.append(file_split[1])\n",
    "        tar_labels.append(file_split[2].split('.')[0])\n",
    "        batch.append(torch.from_numpy(np.load(os.path.join(reference_adv_dir, file))))\n",
    "\n",
    "        img = Image.open(os.path.join('./dataset/images', img_name + '.png'))\n",
    "        if img.mode != 'RGB':\n",
    "            img = img.convert('RGB')\n",
    "        img = transform(img)\n",
    "        imgs.append(img)\n",
    "    \n",
    "    gt_labels = np.array(gt_labels).astype(np.int32)\n",
    "    tar_labels = np.array(tar_labels).astype(np.int32)\n",
    "    gt_labels = torch.from_numpy(gt_labels)\n",
    "    tar_labels = torch.from_numpy(tar_labels)\n",
    "\n",
    "    return torch.stack(batch), gt_labels, tar_labels, torch.stack(imgs)\n",
    "\n",
    "reference_advs_files = os.listdir(reference_adv_dir)\n",
    "num_batches = int(np.ceil(len(reference_advs_files) / batch_size))\n",
    "reference_advs_files_batch = []\n",
    "for i in range(num_batches):\n",
    "    batch = reference_advs_files[i*args.batch_size: min((i+1)*args.batch_size, len(reference_advs_files))]\n",
    "    reference_advs_files_batch.append(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5133d20b-0015-40c8-b33e-ec5d410a1983",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = PascalVOCDataset(\n",
    "    data_root = '../data/VOCdevkit/VOC2012',\n",
    "    data_prefix=dict(\n",
    "        img_path='JPEGImages', seg_map_path='SegmentationClass'),\n",
    "    ann_file='ImageSets/Segmentation/val.txt',\n",
    ")\n",
    "type(val_dataset)\n",
    "len(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f6c862-f070-416d-8150-3c086e32ada3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Custom collate function to filter out None values.\n",
    "    \"\"\"\n",
    "    # Filter out None values from the batch\n",
    "    print (batch)\n",
    "    batch = [{k: v for k, v in item.items() if v is not None} for item in batch]\n",
    "            \n",
    "    # Use default_collate to combine the filtered batch\n",
    "    return torch.utils.data.dataloader.default_collate(batch)\n",
    "\n",
    "# def custom_collate_fn(batch):\n",
    "#     \"\"\"\n",
    "#     Custom collate function to handle None values in the samples.\n",
    "#     \"\"\"\n",
    "#     # Remove keys with None values from each sample\n",
    "#     filtered_batch = []\n",
    "#     for item in batch:\n",
    "#         filtered_item = {k: v for k, v in item.items() if v is not None}\n",
    "#         filtered_batch.append(filtered_item)\n",
    "    \n",
    "#     return filtered_batch\n",
    "\n",
    "val_loader = DataLoader(val_dataset, batch_size=2, collate_fn=custom_collate_fn)\n",
    "next(iter(val_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d1dd77-b001-4afc-9799-e57091a2eb13",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes=['background', 'aeroplane', 'bicycle', 'bird', 'boat',\n",
    "        'bottle', 'bus', 'car', 'cat', 'chair', 'cow', 'diningtable',\n",
    "        'dog', 'horse', 'motorbike', 'person', 'pottedplant', 'sheep',\n",
    "        'sofa', 'train', 'tvmonitor']\n",
    "classes_str = \"\"\n",
    "for name in classes:\n",
    "    classes_str += (name + \", \")\n",
    "classes_str = classes_str[:-2]\n",
    "classes_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1363e841-3c7e-4e53-9fdc-bad853013ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [1, 2]\n",
    "b = [3, 4]\n",
    "bb = []\n",
    "bb.append(torch.tensor(a))\n",
    "bb.append(torch.tensor(b))\n",
    "c = torch.stack(bb, dim=0)\n",
    "c.shape\n",
    "\n",
    "for x, y in zip(a, b):\n",
    "    print (x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb904937",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['MASTER_ADDR'] = 'localhost'\n",
    "os.environ['MASTER_PORT'] = '5678'\n",
    "torch.distributed.init_process_group(\n",
    "    backend='nccl',\n",
    "    world_size=int(os.getenv('WORLD_SIZE', '1')),\n",
    "    rank=int(os.getenv('RANK', '0, 1')),\n",
    ")\n",
    "torch.distributed.get_rank()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4669dfa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "voc_img_base_dir = \"../data/VOCdevkit/VOC2012/JPEGImages\"\n",
    "superpixel_base_dir = \"../data/superpixel_img/scikit30\"\n",
    "val_img_name_list = os.listdir(superpixel_base_dir)\n",
    "\n",
    "img_name = val_img_name_list[0]\n",
    "img = cv2.imread(os.path.join(voc_img_base_dir, img_name.split('.')[0]+\".jpg\"))\n",
    "superpixels = cv2.imread(os.path.join(superpixel_base_dir, img_name))\n",
    "superpixels_ids = np.unique(superpixels)\n",
    "\n",
    "# Gather per superpixel masks\n",
    "superpixels_masks = []\n",
    "for superpixel_id in superpixels_ids:\n",
    "    mask = (superpixels == superpixel_id)\n",
    "    superpixels_masks.append(torch.tensor(mask))\n",
    "\n",
    "superpixels_masks = torch.stack(superpixels_masks, dim=0)\n",
    "superpixels_masks = superpixels_masks.permute(0,3,1,2)\n",
    "color = torch.tensor([[0,0,255], [0,255,0], [255,0,0]]) # R G B\n",
    "# Red = color[0].reshape(1, 3, 1, 1)\n",
    "# Green = color[1].reshape(1, 3, 1, 1)\n",
    "Blue = color[2].reshape(1, 3, 1, 1)\n",
    "\n",
    "# superpixels_masks = superpixels_masks * Red\n",
    "\n",
    "img_repeated = torch.tensor(img).unsqueeze(0).permute(0,3,1,2).expand_as(superpixels_masks)\n",
    "\n",
    "# 图片混合\n",
    "# rendered_imgs = img_repeated * 0.6 + superpixels_masks * Red * 0.4\n",
    "# rendered_imgs = img_repeated * 0.6 + superpixels_masks * Green * 0.4\n",
    "rendered_imgs = img_repeated * 0.6 + superpixels_masks * Blue * 0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd43fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# superpixels_masks[0, 0].shape\n",
    "# img.shape\n",
    "a = torch.tensor([1])\n",
    "a.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0daf3255",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RenderedImageDataset(Dataset):\n",
    "    def __init__(self, rendered_img_dir: str='../data/rendered_img/scikit30'):\n",
    "        super().__init__()\n",
    "        # self.tokenizer = tokenizer\n",
    "        # self.prompt = prompt\n",
    "        self.rendered_img_dir = rendered_img_dir\n",
    "        \n",
    "        self.img_names = os.listdir(rendered_img_dir)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        id_img_path = os.path.join(self.rendered_img_dir, self.img_names[idx])\n",
    "        return id_img_path, self.img_names[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.img_names)\n",
    "\n",
    "\n",
    "val_dataset = RenderedImageDataset()\n",
    "val_loader = DataLoader(val_dataset, batch_size=1)\n",
    "a = next(iter(val_loader))\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341bdd2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.rand(3, 224, 224)\n",
    "for i in range(a.shape[1]):\n",
    "    for j in range(a.shape[2]):\n",
    "        sem_pred[:, i, j] = torch.argmax(torch.bincount(sem_pred_merge[:, i, j]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "38ead205",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/superpixel_img/scikit30/2007_000033.jpg\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = '../data/superpixel_img/scikit30/'\n",
    "img_names = os.listdir(path)\n",
    "img_names.sort()\n",
    "img_path = os.path.join(path, img_names[0])\n",
    "print (img_path)\n",
    "\n",
    "# img = cv2.imread(img_path)\n",
    "# print (img.shape)\n",
    "# img = Image.open(img_path)\n",
    "# img = np.array(img)\n",
    "# print (img.shape)\n",
    "os.path.isdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "edbad8ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes=['background', 'aeroplane', 'bicycle', 'bird', 'boat',\n",
    "        'bottle', 'bus', 'car', 'cat', 'chair', 'cow', 'diningtable',\n",
    "        'dog', 'horse', 'motorbike', 'person', 'pottedplant', 'sheep',\n",
    "        'sofa', 'train', 'tvmonitor']\n",
    "classes_str = \"\"\n",
    "for name in classes:\n",
    "    classes_str += (name + \", \")\n",
    "classes_str = classes_str[:-2]\n",
    "name2id, id2name = dict(), dict()\n",
    "\n",
    "for id, class_name in enumerate(classes):\n",
    "    name2id[class_name] = id\n",
    "    id2name[id] = class_name\n",
    "    \n",
    "name2id\n",
    "'bird' in name2id.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "60a44701",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../data/superpixel_img/scikit30'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = '../data'\n",
    "b = 'superpixel_img'\n",
    "c = 'scikit30'\n",
    "os.path.join(a, b, c)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
