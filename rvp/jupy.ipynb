{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6a5b5d1-40c6-4f4d-bf88-d612e46d07b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/mm/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2023-11-08 13:16:10,629 - modelscope - INFO - PyTorch version 1.13.1+cu117 Found.\n",
      "2023-11-08 13:16:10,633 - modelscope - INFO - Loading ast index from /root/.cache/modelscope/ast_indexer\n",
      "2023-11-08 13:16:10,698 - modelscope - INFO - Loading done! Current index file version is 1.9.4, with md5 15e81bede39dd83de9c6d0777d4ba7ab and a total number of 945 components indexed\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "from skimage.segmentation import slic\n",
    "from fast_slic import Slic\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from mmseg.datasets.transforms import *\n",
    "from mmseg.datasets import PascalVOCDataset\n",
    "from mmengine.structures import PixelData\n",
    "\n",
    "\n",
    "from mmengine.config import Config, DictAction\n",
    "from mmengine.runner import Runner\n",
    "import itertools\n",
    "import time\n",
    "\n",
    "from modelscope import (\n",
    "    snapshot_download, AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac07b566",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model is automatically converting to bf16 for faster inference. If you want to disable the automatic precision, please manually add bf16/fp16/fp32=True to \"AutoModelForCausalLM.from_pretrained\".\n",
      "Loading checkpoint shards: 100%|██████████| 10/10 [00:26<00:00,  2.65s/it]\n"
     ]
    }
   ],
   "source": [
    "model_id = 'qwen/Qwen-VL-Chat'\n",
    "revision = 'v1.0.0'\n",
    "model_dir = '../Qwen-VL-Chat'\n",
    "model = AutoModelForCausalLM.from_pretrained(model_dir, device_map=\"cuda\", trust_remote_code=True).eval()\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_dir, trust_remote_code=True)\n",
    "if not hasattr(tokenizer, 'model_dir'):\n",
    "    tokenizer.model_dir = model_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "20d1dd77-b001-4afc-9799-e57091a2eb13",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes=['background', 'aeroplane', 'bicycle', 'bird', 'boat',\n",
    "        'bottle', 'bus', 'car', 'cat', 'chair', 'cow', 'diningtable',\n",
    "        'dog', 'horse', 'motorbike', 'person', 'pottedplant', 'sheep',\n",
    "        'sofa', 'train', 'tvmonitor']\n",
    "classes_str = \"\"\n",
    "for name in classes:\n",
    "    classes_str += (name + \", \")\n",
    "classes_str = classes_str[:-2]\n",
    "\n",
    "\n",
    "prompt_template = \"<img>{}</img>The green mask in the figure covers part of an object, \\\n",
    "please analyze what is the most likely category of this object? Please select from the categories given below: {}.\\\n",
    "Please distinguish as many categories of objects as possible, and do not be affected by the main objects in the figure.\"\n",
    "\n",
    "name2id, id2name = dict(), dict()\n",
    "\n",
    "for id, class_name in enumerate(classes):\n",
    "    name2id[class_name] = id\n",
    "    id2name[id] = class_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e0ec95f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class FeatureHooker:\n",
    "#     def __init__(self, layer):\n",
    "#         self.layer = layer\n",
    "#         self.fea = None\n",
    "#         self.handle = None\n",
    "\n",
    "#         self.register_hook()\n",
    "    \n",
    "#     def hook_fn(self, m, fea_in, fea_out):\n",
    "#         self.fea = fea_out\n",
    "        \n",
    "#     def register_hook(self):\n",
    "#         self.handle = self.layer.register_forward_hook(self.hook_fn)\n",
    "\n",
    "# hooker = FeatureHooker(layer=model.transformer.visual.transformer.resblocks[47])\n",
    "\n",
    "feature_l = [None]\n",
    "def hook_fn(m, fea_in, fea_out):\n",
    "    feature_l[0] = fea_out.detach().cpu()\n",
    "\n",
    "handle = model.transformer.visual.transformer.resblocks[47].register_forward_hook(hook_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b3f3e115",
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = ['<img>{}</img>Describe this image.'.format(img_path)]\n",
    "responses, history = model.chat(tokenizer=tokenizer, queries=queries, history=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ca3b026c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mask\n"
     ]
    }
   ],
   "source": [
    "img_path = '/remote-home/zhangjiacheng/RVP/data/rendered_img/2007_001175/5.jpg'\n",
    "queries = [prompt_template.format(img_path, classes_str)]\n",
    "responses, history = model.chat(tokenizer=tokenizer, queries=queries, history=None)\n",
    "print (responses[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "96605313",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/mm/lib/python3.9/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n"
     ]
    }
   ],
   "source": [
    "# K-Means Cluter\n",
    "device = 'cuda:0'\n",
    "# feature = hooker.fea[:, 0].cpu().float()\n",
    "feature = feature_l[0][:, 0].float()\n",
    "num_clusters = 6\n",
    "kmeans = KMeans(n_clusters=num_clusters)\n",
    "cluster_ids = kmeans.fit_predict(feature)\n",
    "\n",
    "cluster_img = np.zeros((448, 448, 3))\n",
    "\n",
    "for i in range(32):\n",
    "    for j in range(32):\n",
    "        patch_label = cluster_ids[i * 32 + j]\n",
    "        # color = plt.cm.tab10(patch_label / 10)\n",
    "        # cluster_img[i*14:(i+1)*14, j*14:(j+1)*14] = color[:3]\n",
    "        cluster_img[i*14:(i+1)*14, j*14:(j+1)*14] = patch_label\n",
    "\n",
    "# superpixels_masks.shape: (ids, C, H, W)\n",
    "masks = []\n",
    "for id in np.unique(cluster_ids):\n",
    "    mask = (cluster_img == id)\n",
    "    masks.append(torch.tensor(mask))\n",
    "masks = torch.stack(masks).permute(0, 3, 1, 2)\n",
    "\n",
    "img = cv2.imread(img_path)\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "img = torch.tensor(img.transpose(2, 0, 1))\n",
    "img_repeated = T.Resize((448, 448))(img)[None].expand_as(masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c066165",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'masks' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/remote-home/zhangjiacheng/RVP/rvp/jupy.ipynb 单元格 8\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bdocker/remote-home/zhangjiacheng/RVP/rvp/jupy.ipynb#X42sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m masks\u001b[39m.\u001b[39mshape\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bdocker/remote-home/zhangjiacheng/RVP/rvp/jupy.ipynb#X42sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorchvision\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtransforms\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfunctional\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mTF\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bdocker/remote-home/zhangjiacheng/RVP/rvp/jupy.ipynb#X42sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m resized \u001b[39m=\u001b[39m TF\u001b[39m.\u001b[39mresize(masks, (\u001b[39m500\u001b[39m, \u001b[39m375\u001b[39m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'masks' is not defined"
     ]
    }
   ],
   "source": [
    "masks.shape\n",
    "import torchvision.transforms.functional as TF\n",
    "resized = TF.resize(masks, (500, 375))\n",
    "resized[0, 0]\n",
    "masks[0, 0]\n",
    "\n",
    "t = resized[0, 0].numpy().astype(np.uint8)\n",
    "t.shape\n",
    "plt.imshow(t)\n",
    "# plt.imshow(masks[0, 0].numpy().astype(np.uint8))\n",
    "\n",
    "resized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c49463e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/remote-home/zhangjiacheng/RVP/rvp/jupy.ipynb 单元格 9\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bdocker/remote-home/zhangjiacheng/RVP/rvp/jupy.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m color \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor([[\u001b[39m255\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m], [\u001b[39m0\u001b[39m, \u001b[39m255\u001b[39m, \u001b[39m0\u001b[39m], [\u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m255\u001b[39m]]) \u001b[39m# R G B\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bdocker/remote-home/zhangjiacheng/RVP/rvp/jupy.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m Red \u001b[39m=\u001b[39m color[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mreshape(\u001b[39m1\u001b[39m, \u001b[39m3\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bdocker/remote-home/zhangjiacheng/RVP/rvp/jupy.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m Green \u001b[39m=\u001b[39m color[\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mreshape(\u001b[39m1\u001b[39m, \u001b[39m3\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "color = torch.tensor([[255, 0, 0], [0, 255, 0], [0, 0, 255]]) # R G B\n",
    "Red = color[0].reshape(1, 3, 1, 1)\n",
    "Green = color[1].reshape(1, 3, 1, 1)\n",
    "Blue = color[2].reshape(1, 3, 1, 1)\n",
    "\n",
    "mask_imgs = torch.zeros_like(img_repeated)\n",
    "remain_imgs = torch.zeros_like(img_repeated)\n",
    "mask_imgs[masks] = img_repeated[masks]\n",
    "remain_imgs[masks.logical_not()] = img_repeated[masks.logical_not()]\n",
    "\n",
    "# Render images\n",
    "# Mix-up (Background brightness unchanged)\n",
    "color_mode = 'G'\n",
    "if color_mode == 'R':\n",
    "    rendered_imgs = (mask_imgs * 0.6 + masks * Red * 0.4) + remain_imgs\n",
    "    # rendered_imgs = img_repeated * 0.6 + masks * Red * 0.4\n",
    "elif color_mode == 'G':\n",
    "    rendered_imgs = (mask_imgs * 0.6 + masks * Green * 0.4) + remain_imgs\n",
    "    # rendered_imgs = img_repeated * 0.6 + masks * Green * 0.4\n",
    "elif color_mode == 'B':\n",
    "    rendered_imgs = (mask_imgs * 0.6 + masks * Blue * 0.4) + remain_imgs\n",
    "    # rendered_imgs = img_repeated * 0.6 + masks * Blue * 0.4\n",
    "else:\n",
    "    raise ValueError('Color not supported: {}'.format(color_mode))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e098151b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  1  2  3  4  5 12 13 14 15 16 17 18 19 20 21 22 23]\n",
      "[ 0 18]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1, 375, 500)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = cv2.imread('/remote-home/zhangjiacheng/RVP/data/sem_seg_preds/scikit30/G/2007_001175.jpg')\n",
    "# pred = np.array(Image.open('/remote-home/zhangjiacheng/RVP/data/sem_seg_preds/scikit30/G/2007_001175.jpg'))\n",
    "# print (pred.shape)\n",
    "# print (pred[..., 0])\n",
    "# for i in range(pred.shape[0]):\n",
    "#     print (pred[i])\n",
    "print (np.unique(pred))\n",
    "pred1 = np.load('./test.npy')\n",
    "print (np.unique(pred1))\n",
    "pred1[None].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22,\n",
       "       23], dtype=uint8)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "np.unique(pred2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
